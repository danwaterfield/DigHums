{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-tuning for Burney Attribution\n",
    "\n",
    "This notebook trains a BERT model for 18th-century authorship attribution.\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Runtime → Change runtime type → GPU (T4 or better)\n",
    "2. Upload your `burney-attribution` folder to Colab or mount Google Drive\n",
    "3. Run all cells\n",
    "\n",
    "**Expected time:** 1-2 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.30.0 datasets>=2.12.0 accelerate>=1.1.0 scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Optional)\n",
    "\n",
    "If you've uploaded the data to Google Drive, mount it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Adjust this path to where you uploaded burney-attribution\n",
    "# PROJECT_PATH = '/content/drive/MyDrive/burney-attribution'\n",
    "# import os\n",
    "# os.chdir(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alternative: Upload Data Directly\n",
    "\n",
    "If you don't want to use Google Drive, you can upload the prepared dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if uploading a zip file\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload burney_data.zip\n",
    "# !unzip -q burney_data.zip\n",
    "# !ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Script\n",
    "\n",
    "This is the complete training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BERT AUTHORSHIP ATTRIBUTION TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these paths based on how you uploaded the data\n",
    "DATA_DIR = Path('data/bert_data')  # or '/content/drive/MyDrive/burney-attribution/data/bert_data'\n",
    "OUTPUT_DIR = Path('models/bert_authorship')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mapping\n",
    "print(\"\\nLoading label mapping...\")\n",
    "with open(DATA_DIR / 'label_mapping.json', 'r') as f:\n",
    "    label_info = json.load(f)\n",
    "\n",
    "author_to_id = label_info['author_to_id']\n",
    "id_to_author = {int(k): v for k, v in label_info['id_to_author'].items()}\n",
    "num_labels = len(author_to_id)\n",
    "\n",
    "print(f\"Training for {num_labels} authors: {sorted(author_to_id.keys())}\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "datasets = load_from_disk(str(DATA_DIR / 'chunked_datasets'))\n",
    "\n",
    "print(f\"Train: {len(datasets['train'])} chunks\")\n",
    "print(f\"Val: {len(datasets['validation'])} chunks\")\n",
    "print(f\"Test: {len(datasets['test'])} chunks\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading BERT model...\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_author,\n",
    "    label2id=author_to_id\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text', 'author', 'title', 'year']\n",
    ")\n",
    "\n",
    "tokenized_datasets.set_format('torch')\n",
    "print(\"✓ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "# Training arguments optimized for Colab\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Larger batch on GPU\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,  # Reduced for faster training\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Mixed precision for speed\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "This will take 1-2 hours on a T4 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "trainer.save_model(str(OUTPUT_DIR / 'final'))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR / 'final'))\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "with open(OUTPUT_DIR / 'train_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"✓ Model saved to {OUTPUT_DIR / 'final'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "val_metrics = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {val_metrics['eval_accuracy']:.2%}\")\n",
    "print(f\"Validation F1 (macro): {val_metrics['eval_f1_macro']:.3f}\")\n",
    "print(f\"Validation F1 (weighted): {val_metrics['eval_f1_weighted']:.3f}\")\n",
    "\n",
    "# Detailed predictions\n",
    "val_predictions = trainer.predict(tokenized_datasets['validation'])\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
    "val_labels = val_predictions.label_ids\n",
    "\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(\n",
    "    val_labels,\n",
    "    val_preds,\n",
    "    target_names=sorted(author_to_id.keys()),\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_datasets['test'])\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "test_labels = test_predictions.label_ids\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "test_f1_macro = f1_score(test_labels, test_preds, average='macro')\n",
    "test_f1_weighted = f1_score(test_labels, test_preds, average='weighted')\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Test F1 (macro): {test_f1_macro:.3f}\")\n",
    "print(f\"Test F1 (weighted): {test_f1_weighted:.3f}\")\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    target_names=sorted(author_to_id.keys()),\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'accuracy': float(test_accuracy),\n",
    "    'f1_macro': float(test_f1_macro),\n",
    "    'f1_weighted': float(test_f1_weighted)\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Test results saved to {OUTPUT_DIR / 'test_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_accuracy = 0.80\n",
    "\n",
    "print(f\"Baseline (Burrows' Delta): {baseline_accuracy:.1%}\")\n",
    "print(f\"BERT:                      {test_accuracy:.1%}\")\n",
    "\n",
    "improvement = test_accuracy - baseline_accuracy\n",
    "if improvement > 0:\n",
    "    print(f\"\\n✓ BERT improves on baseline by {improvement:.1%}\")\n",
    "elif improvement < 0:\n",
    "    print(f\"\\n⚠ BERT underperforms baseline by {-improvement:.1%}\")\n",
    "else:\n",
    "    print(f\"\\nBERT matches baseline performance\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results\n",
    "\n",
    "Download the trained model and results back to your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory\n",
    "!zip -r -q bert_model.zip models/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('bert_model.zip')\n",
    "\n",
    "print(\"✓ Download started!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
