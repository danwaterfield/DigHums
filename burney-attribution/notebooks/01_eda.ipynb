{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## 18th-Century Novel Corpus for Burney Attribution\n",
    "\n",
    "This notebook provides an overview of the corpus assembled for training the Burney authorship attribution model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv('../data/metadata.csv')\n",
    "print(f\"Loaded metadata for {len(metadata)} texts\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal texts: {len(metadata)}\")\n",
    "print(f\"Total words: {metadata['word_count'].sum():,}\")\n",
    "print(f\"Unique authors: {metadata['author'].nunique()}\")\n",
    "print(f\"Unique works: {metadata.groupby(['author', 'title']).ngroups}\")\n",
    "print(f\"\\nDate range: {metadata['year'].min()} - {metadata['year'].max()}\")\n",
    "print(f\"\\nAverage words per text: {metadata['word_count'].mean():,.0f}\")\n",
    "print(f\"Median words per text: {metadata['word_count'].median():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution by Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words per author\n",
    "author_words = metadata.groupby('author')['word_count'].agg(['sum', 'count'])\n",
    "author_words.columns = ['total_words', 'num_texts']\n",
    "author_words = author_words.sort_values('total_words', ascending=False)\n",
    "\n",
    "print(\"\\nWords by Author:\")\n",
    "print(author_words.to_string())\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total words by author\n",
    "author_words['total_words'].plot(kind='barh', ax=ax1, color='steelblue')\n",
    "ax1.set_xlabel('Total Words')\n",
    "ax1.set_title('Total Words by Author')\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "\n",
    "# Number of texts by author\n",
    "author_words['num_texts'].plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_xlabel('Number of Texts')\n",
    "ax2.set_title('Number of Texts by Author')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/author_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Class balance analysis\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLASS BALANCE ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "total_words = author_words['total_words'].sum()\n",
    "author_words['percentage'] = (author_words['total_words'] / total_words * 100).round(1)\n",
    "print(\"\\nPercentage of corpus by author:\")\n",
    "print(author_words[['percentage']].to_string())\n",
    "print(f\"\\nBurney represents {author_words.loc['burney', 'percentage']:.1f}% of the corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate multi-volume works for this analysis\n",
    "works = metadata.groupby(['author', 'title', 'year']).agg({\n",
    "    'word_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Timeline plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Color by author\n",
    "authors = works['author'].unique()\n",
    "colors = sns.color_palette('tab10', len(authors))\n",
    "author_colors = dict(zip(authors, colors))\n",
    "\n",
    "for author in authors:\n",
    "    author_data = works[works['author'] == author]\n",
    "    ax.scatter(author_data['year'], \n",
    "              author_data['word_count'], \n",
    "              s=author_data['word_count']/500,  # Size by word count\n",
    "              alpha=0.6,\n",
    "              label=author.title(),\n",
    "              color=author_colors[author])\n",
    "\n",
    "ax.set_xlabel('Publication Year')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Corpus Timeline (bubble size = word count)')\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/temporal_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Burney's career trajectory\n",
    "burney_works = works[works['author'] == 'burney'].sort_values('year')\n",
    "print(\"\\nBurney's Career:\")\n",
    "print(burney_works[['title', 'year', 'word_count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of text lengths\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "metadata['word_count'].hist(bins=20, ax=ax1, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Word Count')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Text Lengths')\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "\n",
    "# Box plot by author\n",
    "metadata.boxplot(column='word_count', by='author', ax=ax2)\n",
    "ax2.set_xlabel('Author')\n",
    "ax2.set_ylabel('Word Count')\n",
    "ax2.set_title('Text Length by Author')\n",
    "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/length_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples from each author\n",
    "def show_sample(author, n_words=200):\n",
    "    \"\"\"Display a sample from an author's text.\"\"\"\n",
    "    file_info = metadata[metadata['author'] == author].iloc[0]\n",
    "    file_path = Path('../data/processed') / file_info['file_path']\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Get first n_words\n",
    "    words = text.split()[:n_words]\n",
    "    sample = ' '.join(words)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{author.upper()}: {file_info['title']} ({file_info['year']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(sample + \"...\")\n",
    "\n",
    "# Show samples from each author\n",
    "for author in sorted(metadata['author'].unique()):\n",
    "    show_sample(author, n_words=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_quality(file_path):\n",
    "    \"\"\"Check for potential quality issues in a text.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check for remaining Gutenberg markers\n",
    "    if 'gutenberg' in text.lower():\n",
    "        issues.append('Contains Gutenberg text')\n",
    "    \n",
    "    # Check for excessive special characters (OCR errors)\n",
    "    special_char_ratio = len(re.findall(r'[^a-zA-Z0-9\\s.,;:!?\\'\"\\-]', text)) / len(text)\n",
    "    if special_char_ratio > 0.01:\n",
    "        issues.append(f'High special char ratio: {special_char_ratio:.2%}')\n",
    "    \n",
    "    # Check for very short texts (possible incomplete)\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 20000:\n",
    "        issues.append(f'Short text: {word_count:,} words')\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Check all texts\n",
    "print(\"Data Quality Report:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "issue_count = 0\n",
    "for _, row in metadata.iterrows():\n",
    "    file_path = Path('../data/processed') / row['file_path']\n",
    "    issues = check_text_quality(file_path)\n",
    "    \n",
    "    if issues:\n",
    "        issue_count += 1\n",
    "        print(f\"\\n{row['author']}/{row['title']}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  ⚠ {issue}\")\n",
    "\n",
    "if issue_count == 0:\n",
    "    print(\"\\n✓ No significant quality issues detected\")\n",
    "else:\n",
    "    print(f\"\\n{issue_count} texts with potential issues (review recommended)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary = metadata.groupby('author').agg({\n",
    "    'title': 'count',\n",
    "    'word_count': ['sum', 'mean', 'min', 'max'],\n",
    "    'year': ['min', 'max']\n",
    "})\n",
    "\n",
    "summary.columns = ['Texts', 'Total Words', 'Avg Words', 'Min Words', 'Max Words', 'First Year', 'Last Year']\n",
    "summary = summary.sort_values('Total Words', ascending=False)\n",
    "\n",
    "# Format numbers\n",
    "for col in ['Total Words', 'Avg Words', 'Min Words', 'Max Words']:\n",
    "    summary[col] = summary[col].apply(lambda x: f\"{int(x):,}\")\n",
    "\n",
    "print(\"\\nComprehensive Summary by Author:\")\n",
    "print(summary.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "summary.to_csv('../outputs/corpus_summary.csv')\n",
    "print(\"\\n✓ Summary saved to outputs/corpus_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Corpus Strengths\n",
    "- Substantial Burney corpus (1.1M words across 4 major works)\n",
    "- Good temporal coverage (1740-1814)\n",
    "- Mix of male and female authors\n",
    "- Contemporary comparison authors in similar genres\n",
    "\n",
    "### Considerations for Model Training\n",
    "1. **Class imbalance**: Burney represents ~44% of corpus. May need to:\n",
    "   - Use stratified sampling\n",
    "   - Apply class weights during training\n",
    "   - Consider data augmentation for minority classes\n",
    "\n",
    "2. **Text length variation**: Some texts much shorter (Edgeworth, Burney volumes). Should:\n",
    "   - Split longer texts into comparable chunks\n",
    "   - Ensure train/val/test split by work (not by chunk)\n",
    "\n",
    "3. **Multi-volume works**: Burney's Cecilia and The Wanderer span multiple files\n",
    "   - Treat as single works for splitting\n",
    "   - Consider whether to train on individual volumes or concatenated\n",
    "\n",
    "4. **Temporal span**: 74-year range\n",
    "   - Language evolution may affect results\n",
    "   - Consider temporal stratification in experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
