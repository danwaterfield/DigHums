{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Authorship Attribution - Azure ML Training\n",
    "\n",
    "Train BERT model with automatic experiment tracking in Azure ML.\n",
    "\n",
    "**Requirements:**\n",
    "- Azure ML Compute Instance with GPU\n",
    "- Data uploaded to compute instance or Azure Blob Storage\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads prepared datasets\n",
    "2. Fine-tunes BERT for 7-author classification\n",
    "3. Logs all metrics to Azure ML automatically\n",
    "4. Saves model to Azure ML Model Registry\n",
    "5. Evaluates on test set\n",
    "\n",
    "**Time:** ~30 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (first time only)\n",
    "!pip install -q transformers datasets scikit-learn mlflow azureml-mlflow\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - adjust if needed\n",
    "PROJECT_DIR = Path.home() / 'cloudfiles' / 'code' / 'DigHums' / 'burney-attribution'\n",
    "DATA_DIR = PROJECT_DIR / 'data' / 'bert_data'\n",
    "OUTPUT_DIR = PROJECT_DIR / 'models' / 'bert_authorship'\n",
    "\n",
    "# Training config\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "EXPERIMENT_NAME = 'burney-attribution'\n",
    "\n",
    "print(f\"üìÇ Project dir: {PROJECT_DIR}\")\n",
    "print(f\"üìä Data dir: {DATA_DIR}\")\n",
    "print(f\"üíæ Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"\\nü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìà Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected! Training will be slow.\")\n",
    "    print(\"   Check: Compute Instance ‚Üí GPU enabled?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Azure ML Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow tracking\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "run = mlflow.start_run()\n",
    "\n",
    "print(f\"‚úÖ Azure ML experiment started: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Run ID: {run.info.run_id}\")\n",
    "print(f\"\\nüìä View progress at: https://ml.azure.com\")\n",
    "print(f\"   Navigate to: Experiments ‚Üí {EXPERIMENT_NAME}\")\n",
    "\n",
    "# Log parameters\n",
    "mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"learning_rate\", LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìñ Loading datasets...\")\n",
    "\n",
    "train_dataset = load_from_disk(str(DATA_DIR / 'chunked_datasets' / 'train'))\n",
    "val_dataset = load_from_disk(str(DATA_DIR / 'chunked_datasets' / 'validation'))\n",
    "test_dataset = load_from_disk(str(DATA_DIR / 'chunked_datasets' / 'test'))\n",
    "\n",
    "# Load label mapping\n",
    "with open(DATA_DIR / 'label_mapping.json', 'r') as f:\n",
    "    label_info = json.load(f)\n",
    "\n",
    "num_labels = len(label_info['author_to_id'])\n",
    "id_to_author = {int(k): v for k, v in label_info['id_to_author'].items()}\n",
    "\n",
    "print(f\"‚úÖ Datasets loaded:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"   Test: {len(test_dataset):,} samples\")\n",
    "print(f\"\\nüìö Authors ({num_labels}): {', '.join(label_info['author_to_id'].keys())}\")\n",
    "\n",
    "# Log dataset info\n",
    "mlflow.log_param(\"num_authors\", num_labels)\n",
    "mlflow.log_param(\"train_size\", len(train_dataset))\n",
    "mlflow.log_param(\"val_size\", len(val_dataset))\n",
    "mlflow.log_param(\"test_size\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ü§ñ Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "# Azure ML logging callback\n",
    "class AzureMLCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        for key, value in logs.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(key, value, step=state.global_step)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',  # Using mlflow directly\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[AzureMLCallback()]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÉ Starting training...\")\n",
    "print(f\"   This will take ~30 minutes on T4 GPU\")\n",
    "print(f\"\\nüìä Watch progress in real-time:\")\n",
    "print(f\"   https://ml.azure.com ‚Üí Experiments ‚Üí {EXPERIMENT_NAME}\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Samples/sec: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Evaluating on test set...\")\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 (weighted): {test_results['eval_f1_weighted']:.4f}\")\n",
    "print(f\"F1 (macro): {test_results['eval_f1_macro']:.4f}\")\n",
    "\n",
    "# Detailed results\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "target_names = [id_to_author[i] for i in range(num_labels)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-AUTHOR PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "report = classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    target_names=target_names,\n",
    "    digits=4,\n",
    "    output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    target_names=target_names,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Log test metrics\n",
    "mlflow.log_metric(\"test_accuracy\", test_results['eval_accuracy'])\n",
    "mlflow.log_metric(\"test_f1_weighted\", test_results['eval_f1_weighted'])\n",
    "mlflow.log_metric(\"test_f1_macro\", test_results['eval_f1_macro'])\n",
    "\n",
    "for author in target_names:\n",
    "    mlflow.log_metric(f\"test_{author}_f1\", report[author]['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_dir = OUTPUT_DIR / 'final'\n",
    "final_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving model to {final_model_dir}...\")\n",
    "trainer.save_model(str(final_model_dir))\n",
    "tokenizer.save_pretrained(str(final_model_dir))\n",
    "\n",
    "# Save label mapping\n",
    "with open(final_model_dir / 'label_mapping.json', 'w') as f:\n",
    "    json.dump(label_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved locally\")\n",
    "\n",
    "# Log to Azure ML Model Registry\n",
    "print(\"\\nüì¶ Logging model to Azure ML Model Registry...\")\n",
    "mlflow.pytorch.log_model(\n",
    "    model,\n",
    "    \"model\",\n",
    "    registered_model_name=\"burney-authorship-attribution\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model logged to Azure ML\")\n",
    "print(\"   Access at: https://ml.azure.com ‚Üí Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Finish Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE ‚úÖ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä View full results:\")\n",
    "print(f\"   https://ml.azure.com\")\n",
    "print(f\"   ‚Üí Experiments ‚Üí {EXPERIMENT_NAME}\")\n",
    "print(f\"\\nüì¶ Model location:\")\n",
    "print(f\"   Local: {final_model_dir}\")\n",
    "print(f\"   Azure ML: Models ‚Üí burney-authorship-attribution\")\n",
    "print(f\"\\nüéØ Test Accuracy: {test_results['eval_accuracy']:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Test on anonymous works:**\n",
    "```python\n",
    "# Run in new notebook or terminal:\n",
    "python scripts/test_anonymous_attribution.py\n",
    "```\n",
    "\n",
    "**Compare experiments:**\n",
    "- Go to Azure ML Studio\n",
    "- Experiments ‚Üí burney-attribution\n",
    "- Select multiple runs\n",
    "- Click \"Compare\"\n",
    "\n",
    "**Deploy as API (optional):**\n",
    "```python\n",
    "# See deploy_model.py for full script\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
